{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from janome.tokenizer import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセット自体の再配布は禁止なので、Gitコミット時に出力を消す\n",
    "def no_redistribution(data):\n",
    "    pass\n",
    "    # return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習コードの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_tsv_path):\n",
    "    df = df = pandas.read_table(dataset_tsv_path)\n",
    "    df = df.dropna(subset=['label'])\n",
    "    print('length: ', len(df))\n",
    "    return df['text'].values, df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  4125\n"
     ]
    }
   ],
   "source": [
    "no_redistribution(load_dataset('/app/data/purin.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['今日', 'の', 'ご飯', 'は', '焼肉', 'です']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "def tokenize(text):\n",
    "    return t.tokenize(text, wakati=True)\n",
    "\n",
    "list(tokenize(\"今日のご飯は焼肉です\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import count_nonzero\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, x_train, y_train, x_test, y_test,\n",
    "        vectorizer=CountVectorizer()) -> None:\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "    def train(self):\n",
    "        x_train_vec = self.vectorizer.fit_transform(self.x_train)\n",
    "        # 不均衡データセットなので重みをつける\n",
    "        weights = {\n",
    "            0: 1 / count_nonzero(y_train == 0),\n",
    "            1: 1 / count_nonzero(y_train == 1),\n",
    "        }\n",
    "        self.model = LogisticRegression(solver='liblinear', class_weight=weights)\n",
    "        self.model.fit(x_train_vec, self.y_train)\n",
    "        return self.model\n",
    "\n",
    "    def predict(self, x, proba = True):\n",
    "        x_vec = self.vectorizer.transform(x)\n",
    "        if proba:\n",
    "            predictor = self.model.predict_proba\n",
    "        else:\n",
    "            predictor = self.model.predict\n",
    "        return predictor(x_vec)\n",
    "\n",
    "    def eval_and_print(self):\n",
    "        y_pred = self.predict(self.x_test, proba=False)\n",
    "        print('roc_auc_score (macro average)', roc_auc_score(self.y_test, y_pred, average='macro'))\n",
    "        tn, fp, fn, tp = confusion_matrix(self.y_test, y_pred).ravel()\n",
    "        print('tn, fp, fn, tp', tn, fp, fn, tp)\n",
    "        print(classification_report(self.y_test, y_pred))\n",
    "    \n",
    "    def trailn_and_eval(self):\n",
    "        self.train()\n",
    "        self.eval_and_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  4125\n"
     ]
    }
   ],
   "source": [
    "x, y = load_dataset('/app/data/purin.tsv')\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "trainer = Trainer(x_train, y_train, x_test, y_test, vectorizer=CountVectorizer(tokenizer=tokenize))\n",
    "\n",
    "model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score (macro average) 0.7891548948751643\n",
      "tn, fp, fn, tp 54 10 202 559\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.84      0.34        64\n",
      "           1       0.98      0.73      0.84       761\n",
      "\n",
      "    accuracy                           0.74       825\n",
      "   macro avg       0.60      0.79      0.59       825\n",
      "weighted avg       0.92      0.74      0.80       825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.eval_and_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46502289, 0.53497711]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(['ポケモンのプリンとニャースとイーブイかわいい'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50582301, 0.49417699]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(['セブンのスイーツの焼きプリンおいしい'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理の比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score (macro average) 0.7891548948751643\n",
      "tn, fp, fn, tp 54 10 202 559\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.84      0.34        64\n",
      "           1       0.98      0.73      0.84       761\n",
      "\n",
      "    accuracy                           0.74       825\n",
      "   macro avg       0.60      0.79      0.59       825\n",
      "weighted avg       0.92      0.74      0.80       825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Trainer(x_train, y_train, x_test, y_test, vectorizer=CountVectorizer(tokenizer=tokenize)).trailn_and_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter特有表現の除去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score (macro average) 0.7977681504599212\n",
      "tn, fp, fn, tp 53 11 177 584\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.83      0.36        64\n",
      "           1       0.98      0.77      0.86       761\n",
      "\n",
      "    accuracy                           0.77       825\n",
      "   macro avg       0.61      0.80      0.61       825\n",
      "weighted avg       0.92      0.77      0.82       825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def remove_anchor(text):\n",
    "    return re.sub(r'\\@[a-zA-Z0-9]+', '', text)\n",
    "\n",
    "Trainer(x_train, y_train, x_test, y_test,\n",
    "    vectorizer=CountVectorizer(tokenizer=tokenize, preprocessor=remove_anchor)).trailn_and_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score (macro average) 0.785869743758213\n",
      "tn, fp, fn, tp 54 10 207 554\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.84      0.33        64\n",
      "           1       0.98      0.73      0.84       761\n",
      "\n",
      "    accuracy                           0.74       825\n",
      "   macro avg       0.59      0.79      0.58       825\n",
      "weighted avg       0.92      0.74      0.80       825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# minimum document frequency. see: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "Trainer(x_train, y_train, x_test, y_test,\n",
    "    vectorizer=CountVectorizer(tokenizer=tokenize, min_df=3)).trailn_and_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score (macro average) 0.800108820630749\n",
      "tn, fp, fn, tp 57 7 221 540\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.89      0.33        64\n",
      "           1       0.99      0.71      0.83       761\n",
      "\n",
      "    accuracy                           0.72       825\n",
      "   macro avg       0.60      0.80      0.58       825\n",
      "weighted avg       0.93      0.72      0.79       825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Trainer(x_train, y_train, x_test, y_test,\n",
    "    vectorizer=CountVectorizer(tokenizer=tokenize, max_df=0.7)).trailn_and_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ストップワード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -o /app/tmp/stopwords.txt http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/purin-nlp-9TtSrW0h-py3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['あ', 'う', 'か', 'かや', 'き', 'さ', 'しよ', 'た', 'な', 'の', 'ま', 'カ', '生'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score (macro average) 0.7878408344283838\n",
      "tn, fp, fn, tp 54 10 204 557\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.84      0.34        64\n",
      "           1       0.98      0.73      0.84       761\n",
      "\n",
      "    accuracy                           0.74       825\n",
      "   macro avg       0.60      0.79      0.59       825\n",
      "weighted avg       0.92      0.74      0.80       825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stopwords = pandas.read_csv('/app/tmp/stopwords.txt', header=None)\n",
    "stopwords = list(df_stopwords[0])\n",
    "Trainer(x_train, y_train, x_test, y_test,\n",
    "    vectorizer=CountVectorizer(tokenizer=tokenize, stop_words=stopwords)).trailn_and_eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score (macro average) 0.8133931504599212\n",
      "tn, fp, fn, tp 55 9 177 584\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.86      0.37        64\n",
      "           1       0.98      0.77      0.86       761\n",
      "\n",
      "    accuracy                           0.77       825\n",
      "   macro avg       0.61      0.81      0.62       825\n",
      "weighted avg       0.93      0.77      0.82       825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Trainer(x_train, y_train, x_test, y_test,\n",
    "    vectorizer=TfidfVectorizer(tokenizer=tokenize)).trailn_and_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight={0: 0.004484304932735426,\n",
       "                                 1: 0.00032499187520311994},\n",
       "                   solver='liblinear')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_trainer = Trainer(x_train, y_train, x_test, y_test, vectorizer=TfidfVectorizer(tokenizer=tokenize))\n",
    "tfidf_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49974796, 0.50025204]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_trainer.predict(['プリンとイーブイかわいい'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50026531, 0.49973469]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_trainer.predict(['焼きプリンおいしい'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "028baece2d5c9c450a926b56a2feb9be08d5da082e5e98dbd5d7f0ec42d420e8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('purin-nlp-9TtSrW0h-py3.10': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
